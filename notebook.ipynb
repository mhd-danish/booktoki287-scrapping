{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89fef71d-80fd-47eb-aa2b-38aff84f3563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from googletrans import Translator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71aaef68-2b22-4795-8f1b-0204d459963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Translate text\n",
    "# target_language = 'en'\n",
    "# translator = Translator()\n",
    "# result = translator.translate(content[1].text, dest=target_language)\n",
    "\n",
    "# # Print result\n",
    "# print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be6c8a3-1475-45a2-aea6-652bd38db8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23375d5-4a4d-4a34-8acd-23b39dd14fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d434e7-f0d9-4130-ae7c-420f1be5cb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad5024a9-990e-4ea9-8cbe-b12a7b630dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawler:\n",
    "    '''\n",
    "    A webscrapper object that is capable of scrapping novels on `https://booktoki287.com` iteratively through selenium webdriver.\n",
    "    '''\n",
    "    def __init__(self, initial_chapter_url, headless=True):\n",
    "        \n",
    "        self.paragraphs = None\n",
    "        self.next_ch_url = None\n",
    "        self.status = False\n",
    "        # Configure webdriver options\n",
    "        options = Options()\n",
    "\n",
    "        # Disable image loading\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_experimental_option(\n",
    "            \"prefs\", {\"profile.managed_default_content_settings.images\": 2}\n",
    "        )\n",
    "        \n",
    "        # Do not show the web window\n",
    "        if headless:\n",
    "            options.add_argument('--headless')\n",
    "            \n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "        try:\n",
    "            # Initialize webdriver and get the page\n",
    "            self.driver.get(initial_chapter_url)\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Get the page source and parse it with BeautifulSoup\n",
    "            html_content = self.driver.page_source\n",
    "            self.soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            # Your scraping logic here\n",
    "            self.content = self.soup.find('div', {'class', 'view-padding'})\n",
    "            self.content = self.soup.find_all('p')\n",
    "\n",
    "            self.__ch_num = 1\n",
    "            self.get_ch_num()\n",
    "            \n",
    "        except:\n",
    "            self.quit()\n",
    "        \n",
    "    def get_ch_num(self):\n",
    "        '''\n",
    "        Get the chapter number scrapped from the current html content.\n",
    "        '''\n",
    "        self.ch_num = self.soup.find('div', {'class', 'toon-title'}).find_all('span')[0].text.split('/')[0]\n",
    "        self.ch_num = re.sub(\"\\D\", '', self.ch_num)\n",
    "        \n",
    "    def get_next_chapter(self):\n",
    "        '''\n",
    "        Get the html content for the next chapter.\n",
    "        '''\n",
    "        if self.__ch_num == 1:\n",
    "            self.paragraphs = self.content\n",
    "            self.__ch_num = None\n",
    "        else:\n",
    "            self.next_ch_url = self.soup.find('a', id='goNextBtn')['href']\n",
    "            if 'next' in self.next_ch_url:\n",
    "                self.status = True\n",
    "                return 'The scrapper has run successfully'\n",
    "            self.driver.get(self.next_ch_url)\n",
    "            # time.sleep(2)\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Get the page source and parse it with BeautifulSoup\n",
    "            html_content = self.driver.page_source\n",
    "            self.soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            # Your scraping logic here\n",
    "            self.content = self.soup.find('div', {'class', 'view-padding'})\n",
    "            self.content = self.soup.find_all('p')\n",
    "            \n",
    "            self.paragraphs = self.content\n",
    "            self.get_ch_num()\n",
    "                \n",
    "    def __call__(self, save_dir: str=None):\n",
    "\n",
    "        # change the current working directory\n",
    "        os.chdir('C:\\\\Users\\\\muham\\\\OneDrive\\\\Desktop\\\\Dataset for finetuning MTL-LLM\\\\')\n",
    "\n",
    "        # if the save_in path does not exist then create it first\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        while not self.status:\n",
    "            self.get_next_chapter()\n",
    "            clear_output()\n",
    "            # create file_name for the text file\n",
    "            file_name = os.path.join(save_dir, str(self.ch_num)+'.txt')\n",
    "\n",
    "            # save the `content` in the text file iteratively.\n",
    "            with open(file_name, 'w', encoding='utf-8') as f:\n",
    "                for p in self.paragraphs:\n",
    "                    f.write(f\"{p.text}\\n\")\n",
    "\n",
    "            print(f\"Done: {self.ch_num}\")\n",
    "            \n",
    "        \n",
    "    def quit(self):\n",
    "        '''\n",
    "        Quit the selenium webdriver.\n",
    "        '''\n",
    "        self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3061a704-7bbb-42a4-bd7b-fa9b9434c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = crawler(\"https://booktoki287.com/novel/4456323\", headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "346179a8-771e-4b3f-90df-40be4dd2a5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 628\n"
     ]
    }
   ],
   "source": [
    "test(save_dir='rdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90871b-feeb-4c22-9834-6adaf157fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d3bda-58ae-44e5-8064-61ef71e49772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e517e813-81c8-487b-b52c-adcd8147c891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a441100c-8828-4eb4-b177-bfde83e3cfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8715006-d8ca-48eb-9d02-bf1b6d2c5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset\\\\1.txt\", 'r', encoding='utf-8') as f:\n",
    "    a = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe825486-3994-4f14-848c-4e51e2e87b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"translated_chapters\\\\1.txt\", 'r') as f:\n",
    "    b = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f0559f1-5035-4b7b-a8b8-8c14c90d7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4b5eb44-d297-42cf-9908-8c147a5ed8cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m org \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraws\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranslations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3\\lib\\site-packages\\pandas\\core\\frame.py:662\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    656\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    657\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    658\u001b[0m     )\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    661\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 662\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:666\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    664\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    670\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    671\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "org = pd.DataFrame({'raws': a[1:], 'translations': b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faed345-e3d6-4d21-8886-0578fadc31cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad27db-3618-46e5-9e6a-ae81961a539b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
